/**
 * PERFUME AI â€“ å–®æª”å®Œæ•´ç‰ˆ
 * Ollama â†’ é¦™æ°›é‹ç®— / RAG â†’ ä¸è¡Œå†è½‰ OpenAI GPT
 */

import express from "express";
import cors from "cors";
import fs from "fs";
import dotenv from "dotenv";
dotenv.config();

const app = express();
app.use(cors());
app.use(express.json({ limit: "2mb" }));

/* =======================
   åŸºæœ¬è¨­å®š
======================= */
const PORT = 5050;

const OLLAMA_HOST = "http://127.0.0.1:11434";
const CHAT_MODEL = "qwen2.5:7b";
const EMBED_MODEL = "nomic-embed-text";

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const OPENAI_MODEL = "gpt-5.2";

const STORE_FILE = "./rag_store.json";

/* =======================
   åˆå§‹åŒ–çŸ¥è­˜åº«
======================= */
if (!fs.existsSync(STORE_FILE)) {
  fs.writeFileSync(
    STORE_FILE,
    JSON.stringify(
      {
        chunks: [
          {
            text: `
ã€é¦™æ°›é‹ç®—æ ¸å¿ƒè¦å‰‡ã€‘
1. é…æ–¹ç¸½å’Œå¿…é ˆ = 100%
2. å¸¸è¦‹æ¯”ä¾‹ï¼š
   - æ¸…æ–°å‹ï¼šå‰èª¿40% ä¸­èª¿40% å¾Œèª¿20%
   - èŠ±é¦™å‹ï¼šå‰25% ä¸­50% å¾Œ25%
   - æœ¨è³ªå‹ï¼šå‰15% ä¸­35% å¾Œ50%
3. è‹¥ä½¿ç”¨è€…æœªæŒ‡å®š mlï¼Œé è¨­ä»¥ 10ml é¦™ç²¾è¨ˆç®—
4. å®‰å…¨æç¤ºï¼š
   - å­•å©¦ / å¬°å¹¼å…’ï¼šé¿å…é«˜æ¿ƒåº¦è–„è·ã€å°¤åŠ åˆ©
   - æ•æ„Ÿé«”è³ªï¼šç¸½æ¿ƒåº¦ â‰¤ 5%
5. æ„Ÿè¦º â†’ åŸæ–™éª¨æ¶ï¼š
   - ä¹¾æ·¨ï¼šä½›æ‰‹æŸ‘ + ç™½èŠ± + ç™½éºé¦™
   - æ”¾é¬†ï¼šè–°è¡£è‰ + é›ªæ¾
   - é«˜ç´šï¼šç«ç‘° + ä¾è˜­ + æª€é¦™
`
          }
        ]
      },
      null,
      2
    )
  );
}

/* =======================
   å·¥å…·å‡½å¼
======================= */
function loadStore() {
  return JSON.parse(fs.readFileSync(STORE_FILE, "utf-8"));
}

async function embed(text) {
  const r = await fetch(`${OLLAMA_HOST}/api/embeddings`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ model: EMBED_MODEL, prompt: text })
  });
  const d = await r.json();
  return d.embedding;
}

function cosine(a, b) {
  let dot = 0,
    na = 0,
    nb = 0;
  for (let i = 0; i < a.length; i++) {
    dot += a[i] * b[i];
    na += a[i] * a[i];
    nb += b[i] * b[i];
  }
  return dot / (Math.sqrt(na) * Math.sqrt(nb) + 1e-9);
}

/* =======================
   é¦™æ°› RAG + åˆ¤æ–·
======================= */
async function askOllama(userQuestion) {
  const store = loadStore();
  const qv = await embed(userQuestion);

  const scored = store.chunks.map(c => ({
    text: c.text,
    score: cosine(qv, c.embedding ?? qv) // é¦–æ¬¡ä¿éšª
  }));

  const context = scored
    .sort((a, b) => b.score - a.score)
    .slice(0, 3)
    .map((c, i) => `ã€çŸ¥è­˜${i + 1}ã€‘\n${c.text}`)
    .join("\n\n");

  const system = `
ä½ æ˜¯ã€ŒMORAN é¦™æ°›èª¿é¦™ AIã€ã€‚
ä½ å¿…é ˆæ ¹æ“šçŸ¥è­˜çµ¦å‡ºã€Œå¯å¯¦éš›èª¿é¦™çš„æ¯”ä¾‹èˆ‡æ­¥é©Ÿã€ã€‚
å¦‚æœè³‡è¨Šä¸è¶³ï¼Œè«‹å›ç­” answerable=falseã€‚
è¼¸å‡ºæ ¼å¼ä¸€å®šæ˜¯ JSONã€‚
`;

  const prompt = `
${context}

ä½¿ç”¨è€…å•é¡Œï¼š
${userQuestion}

è«‹è¼¸å‡ºï¼š
{
  "answerable": true/false,
  "answer": "...",
  "confidence": 0~1
}
`;

  const r = await fetch(`${OLLAMA_HOST}/api/generate`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: CHAT_MODEL,
      system,
      prompt,
      format: "json",
      stream: false
    })
  });

  const d = await r.json();
  try {
    return JSON.parse(d.response);
  } catch {
    return { answerable: false, confidence: 0 };
  }
}

/* =======================
   OpenAI Fallback
======================= */
async function askGPT(question) {
  const r = await fetch("https://api.openai.com/v1/responses", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${OPENAI_API_KEY}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify({
      model: OPENAI_MODEL,
      input: question
    })
  });

  const d = await r.json();
  return d.output_text || "GPT ç„¡å›æ‡‰";
}

/* =======================
   APIï¼ˆModelView åªæ¥é€™ï¼‰
======================= */
app.post("/api/chat", async (req, res) => {
  const { message } = req.body;

  const local = await askOllama(message);

  if (local.answerable && local.confidence >= 0.4) {
    return res.json({
      route: "ollama",
      answer: local.answer
    });
  }

  const gpt = await askGPT(message);
  res.json({
    route: "gpt",
    answer: gpt
  });
});

/* =======================
   å•Ÿå‹•
======================= */
app.listen(PORT, () => {
  console.log(`ğŸŒ¸ Perfume AI running at http://localhost:${PORT}`);
});
